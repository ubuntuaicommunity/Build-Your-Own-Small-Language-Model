# GDM Lab Series ‚Äì Statistical Language Models & N-Gram Foundations

This repository contains four practical labs introducing the foundations of **probability**, **n-gram language models**, **dataset preparation**, and the comparison between **classical statistical models** and **modern transformer-based models**.  
These labs form part of a foundational module on **Generative Data Modelling (GDM)**.

---

## üìÇ Repository Structure

### **1. Lab 1.1 ‚Äì Create Your Own Probability Distribution**  
**File:** `_gdm_lab_1_1_create_your_own_probability_distribution.ipynb`  
Learn the basics of:
- Constructing discrete probability distributions  
- Sampling and visualization  
- Understanding how probability underpins statistical language models  

---

### **2. Lab 1.2 ‚Äì Experiment With N-Gram Models**  
**File:** `gdm_lab_1_2_experiment_with_n_gram_models.ipynb`  
Explore:
- Unigrams, bigrams, trigrams  
- Building an n-gram model from scratch  
- Conditional probabilities  
- Generating text using n-gram models  
- How performance changes as *n* increases  

---

### **3. Lab 1.3 ‚Äì Compare N-Gram Models & Transformer Language Models**  
**File:** `_gdm_lab_1_3_compare_n_gram_models_and_transformer_language_models.ipynb`  
Understand:
- Key differences between SLMs and Transformers  
- Why context length matters  
- Evaluation using metrics such as perplexity  
- Practical limitations of statistical models  

---

### **4. Lab 1.4 ‚Äì Prepare a Dataset for Training a Statistical LM**  
**File:** `_gdm_lab_1_4_prepare_the_dataset_for_training_a_slm.ipynb`  
Covers:
- Text cleaning and normalization  
- Tokenization approaches  
- Preparing train/test splits  
- Building a dataset ready for SLM experimentation  

---

## üéØ Learning Objectives

After completing this lab series, you will be able to:

- Build and analyze probability distributions  
- Implement your own n-gram language model  
- Compare classical n-gram approaches to transformer-based LLMs  
- Prepare datasets for language modelling tasks  
- Understand the evolution from statistical to neural language models  

---

## üõ†Ô∏è Prerequisites

- Python 3.8+  
- Jupyter Notebook or JupyterLab  

Install required libraries:

```bash
pip install numpy pandas matplotlib nltk transformers
